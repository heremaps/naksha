# Configuration for Amazon r6idn.metal
# This configuration can be used by simply adding these lines to the postgres.conf:
#
# ----------------------------------------------------------------------------------------

# Memory/Disk
max_connections = 1024
shared_buffers = 512GB
maintenance_work_mem = 8GB
default_statistics_target = 500
work_mem = 256MB

# WAL
# To change WAL segment size on existing system, do:
# pg_resetwal -D "$PGDATA" --wal-segsize 1GB
wal_level = minimal
wal_compression = lz4    # off, pglz, lz4 (--with-zstd) or zstd (--with-zstd)
full_page_writes = off   # When this parameter is on, the PostgreSQL server writes the entire content
                         # of each disk page to WAL during the first modification of that page after a
                         # checkpoint.
                         # This option assumes that there are none atomic writes, so that partial
                         # writes of a row is possible. However, we aligned the chunk-size (strip-size)
                         # with the row size and that makes is close to impossible to get partial writes
                         # for our EBS volumes. The impact of this setting with our high frequency of
                         # WAL log writes makes it by far to expensive, especially taking into consideration
                         # that we anyway have backups as snapshots on daily base.
#wal_segment_size = 1GB #  The total size of a WAL segment file in bytes (defined in initdb).
wal_buffers = 1GB # The amount of shared memory used for WAL data that has not yet been written to disk (max wal_segment_size).
min_wal_size = 64GB # Minimum WAL size, at least two times wal_segment_size
max_wal_size = 768GB # Maximum WAL size, at least min_wal_size
                      # xlog is started when (max_wal_size / (2 + checkpoint_completion_target)) hit
                      # see: https://github.com/postgres/postgres/blob/REL_16_STABLE/src/backend/access/transam/xlog.c#L1947
                      # #define ConvertToXSegs(x, segsize)	XLogMBVarToSegs((x), (segsize))
                      # ...
                      # static void CalculateCheckpointSegments(void) {
                      # target = (double) ConvertToXSegs(max_wal_size_mb, wal_segment_size) / (1.0 + CheckPointCompletionTarget);
                      # In a nutshell:
                      #     max_wal_size / (1 + checkpoint_completion_target), so around 50% of max_wal_size
                      # This means, to ensure that we do not trigger xlog too early we need to delay it until
                      # the last moment, which is for max_wal_size = (shared_buffers * 1.5), resulting in xlog
                      # starting at (shared_buffers * 0.75)
                      # In our case, xlog we be trigger after around 384gb have been written
                      # (if network is 100% utilized, after around 16 to 20 seconds, roughly)
wal_keep_size = 0 # We do not have replicas
max_wal_senders = 0 # default is 10, prevents to set WAL to minimal
# A checkpoint moves WAL log forward and writes dirty pages.
# We want to write at least 90% of dirty shared_buffers in the checkpoint timeout
# We have 100Gbps EBS bandwidth and 200Gbps network, so theoretically, we can collect 25gb/s of dirty pages
# Assuming we want to bulk load 100 million features with 300gb (around 3kb/feature)
# - We want it to be loaded at ones and then later being written down to EBS
# - We can write 12.5gb/s to WAL and keep all dirty pages in memory
# - It will take only 24 seconds to be loaded, sp 60s is totally long enough, even for slower clients
# Assuming we need to write WAL logs and dirty pages, we have only have 50% of throughput, 6.25gb/s
# - With 6.25gb/s we can write 187.5gb in 30 seconds
checkpoint_completion_target = 0.9
checkpoint_timeout = 60s
# wal-writer runs only when synchronous_commit=off
wal_writer_delay = 1000
wal_writer_flush_after = 1GB
# Write dirty pages in background, reduces load to WAL writer.
# Normally, write 2 times the pages that have been newly allocated up to max-pages.
#bgwriter_delay = 1000ms
#bgwriter_lru_multiplier = 2
bgwriter_lru_maxpages = 0 # Disable background writer, we use only checkpoints, but these more frequently
#bgwriter_flush_after = 0
# disable explicit flush, OS shall do by itself
# write-throughput = (32*320000)/1000 = 10240 mb/s
# total pages = 256*2^30/32,000 = 8,388,608
# write-throughput = ({page-size}*{bgwriter_lru_maxpages})/{bgwriter_delay} = around mb/s
# total pages = {shared_buffers in GiB}*2^20/{page-size}

# Concurrency
effective_io_concurrency = 1000
max_worker_processes = 256
max_parallel_workers = 256
max_parallel_workers_per_gather = 32
max_parallel_maintenance_workers = 8
archive_mode = off

# Planner cost
effective_cache_size = 896GB    # Sets the planner's assumption about the total size of the data caches
seq_page_cost = 1.0000			    # Sets the planner's estimate of the cost of a sequentially fetched disk page
random_page_cost = 1.0500			  # Sets the planner's estimate of the cost of a non sequentially fetched disk page
cpu_tuple_cost = 0.0100			    # Sets the planner's estimate of the cost of processing each tuple
cpu_index_tuple_cost = 0.0001		# Sets the planner's estimate of the cost of processing each index entry during an index scan
                                # 1/100'th of cpu_tuple_cost, we want index to be used always
cpu_operator_cost = 0.0025		  # Sets the planner's estimate of the cost of processing each operator or function call
parallel_setup_cost = 100.0	    # Sets the planner's estimate of the cost of starting up worker processes for parallel query
parallel_tuple_cost = 0.0100	  # Sets the planner's estimate of the cost of passing each tuple (row) from worker to leader backend
min_parallel_table_scan_size = 1MB
min_parallel_index_scan_size = 1kB

# - Genetic Query Optimizer -

#geqo = on
#geqo_threshold = 12
#geqo_effort = 5			# range 1-10
#geqo_pool_size = 0			# selects default based on effort
#geqo_generations = 0			# selects default based on effort
#geqo_selection_bias = 2.0		# range 1.5-2.0
#geqo_seed = 0.0			# range 0.0-1.0

# - Other Planner Options -

#default_statistics_target = 100	# range 1-10000
#constraint_exclusion = partition	# on, off, or partition
#cursor_tuple_fraction = 0.1		# range 0.0-1.0
#from_collapse_limit = 8
jit = off				# allow JIT compilation
#join_collapse_limit = 8		# 1 disables collapsing of explicit
					# JOIN clauses
#plan_cache_mode = auto			# auto, force_generic_plan or
					# force_custom_plan
#recursive_worktable_factor = 10.0	# range 0.001-1000000

row_security = off
#default_table_access_method = 'heap'
#default_tablespace = ''		        # a tablespace name, '' uses the default
default_toast_compression = 'lz4'	  # 'pglz' or 'lz4'
temp_tablespaces = 'temp'			      # a list of tablespace names, '' uses
					                          # only default tablespace
#check_function_bodies = on
default_transaction_isolation = 'serializable' # serializable, repeatable read, read committed, read uncommitted
#default_transaction_read_only = off
#default_transaction_deferrable = off
#session_replication_role = 'origin'
statement_timeout = 5min			              # in milliseconds, 0 is disabled
                                            # Abort any statement that takes more than the specified amount of time.
lock_timeout = 10s			                    # in milliseconds, 0 is disabled
idle_in_transaction_session_timeout = 60s   # in milliseconds, 0 is disabled
                                            # Sets the maximum allowed idle time between queries, when in a transaction
                                            # Terminate any session that has been idle (that is, waiting for a client query)
                                            # within an open transaction for longer than the specified amount of time.
idle_session_timeout = 15min                # in milliseconds, 0 is disabled
                                            # Sets the maximum allowed idle time between queries, when not in a transaction
                                            # Terminate any session that has been idle (that is, waiting for a client query),
                                            # but not within an open transaction, for longer than the specified amount of time.
                                            # If this value is specified without units, it is taken as milliseconds.
gin_pending_list_limit = 16MB               # Sets the maximum size of a GIN index's pending list, which is used when fastupdate
                                            # is enabled. If the list grows larger than this maximum size, it is cleaned up by
                                            # moving the entries in it to the index's main GIN data structure in bulk.
#createrole_self_grant = ''		# set and/or inherit

#------------------------------------------------------------------------------
# STATISTICS
#------------------------------------------------------------------------------

# - Cumulative Query and Index Statistics -

track_activities = on                 # Collects information about executing commands (default=on)
track_activity_query_size = 1MB	      # Sets the size reserved for pg_stat_activity.query, in bytes (max 1mb)
track_counts = on                     # Collects statistics on database activity (default=on)
track_io_timing = on                  # Collects timing statistics for database I/O activity (default=off)
track_wal_io_timing = on              # Collects timing statistics for WAL I/O activity (default=off)
track_functions = all			            # none, pl, all (default=none)
                                      # Collects function-level statistics on database activity
stats_fetch_consistency = none	      # cache, none, snapshot (default=cache)
                                      # Sets the consistency of accesses to statistics data
                                      # When set to none, each access re-fetches counters from shared memory.
                                      # When set to cache, the first access to statistics for an object caches
                                      # those statistics until the end of the transaction unless
                                      # pg_stat_clear_snapshot() is called. When set to snapshot, the first
                                      # statistics access caches all statistics accessible in the current database,
                                      # until the end of the transaction unless pg_stat_clear_snapshot() is called.


# - Monitoring -

compute_query_id = auto               # Enables in-core computation of a query identifier. Query identifiers can
                                      # be displayed in the pg_stat_activity view, using EXPLAIN, or emitted in
                                      # the log if configured via the log_line_prefix parameter.
# All are by default off
log_statement_stats = on              # Writes cumulative performance statistics to the server log
log_parser_stats = off                # Writes parser performance statistics to the server log
log_planner_stats = off               # Writes planner performance statistics to the server log
log_executor_stats = off              # Writes executor performance statistics to the server log
log_autovacuum_min_duration = 2s      # log autovacuum activity;
                                      # -1 disables, 0 logs all actions and their durations,
                                      # > 0 logs only actions running at least this number of milliseconds.

#------------------------------------------------------------------------------
# ()AUTO)VACUUM
#------------------------------------------------------------------------------

vacuum_cost_delay = 0                         # milliseconds; The amount of time that the process will sleep when the cost
                                              # limit has been exceeded. If this value is specified without units, it is
                                              # taken as milliseconds. The default value is zero, which disables the
                                              # cost-based vacuum delay feature. Positive values enable cost-based vacuuming.
vacuum_cost_limit = 1000                      # The accumulated cost that will cause the vacuuming process to sleep.
autovacuum = on			                          # Enable autovacuum subprocess?  'on'
					                                    # requires track_counts to also be on.
autovacuum_max_workers = 32		                # max number of autovacuum subprocesses
autovacuum_naptime = 10s		                  # time between autovacuum runs
autovacuum_vacuum_threshold = 10000	          # min number of row updates before vacuum
autovacuum_vacuum_insert_threshold = 100000	  # min number of row inserts before vacuum; -1 disables insert vacuums
autovacuum_analyze_threshold = 10000	        # min number of row updates before analyze
autovacuum_vacuum_scale_factor = 0.1	        # fraction of table size before vacuum
autovacuum_vacuum_insert_scale_factor = 0.1	  # fraction of inserts over table size before insert vacuum
autovacuum_analyze_scale_factor = 0.05        # fraction of table size before analyze
#autovacuum_freeze_max_age = 200000000	        # maximum XID age before forced vacuum
#autovacuum_multixact_freeze_max_age = 400000000	  # maximum multixact age before forced vacuum
autovacuum_vacuum_cost_delay = 0    	        # milliseconds; default vacuum cost delay for autovacuum, -1 means use vacuum_cost_delay					                                    #
autovacuum_vacuum_cost_limit = 1000	          # default vacuum cost limit for autovacuum, -1 means use vacuum_cost_limit
                                              # Vacuum cost amount available before napping, for autovacuum
#vacuum_freeze_table_age = 150000000
#vacuum_freeze_min_age = 50000000
#vacuum_failsafe_age = 1600000000
#vacuum_multixact_freeze_table_age = 150000000
#vacuum_multixact_freeze_min_age = 5000000
#vacuum_multixact_failsafe_age = 1600000000


#------------------------------------------------------------------------------
# LOCK MANAGEMENT
#------------------------------------------------------------------------------

deadlock_timeout = 5s                       # This is the amount of time to wait on a lock before checking to see if there is
                                            # a deadlock condition. The check for deadlock is relatively expensive, so the
                                            # server doesn't run it every time it waits for a lock.
max_locks_per_transaction = 256		          # min 10
max_pred_locks_per_transaction = 256	      # min 10
max_pred_locks_per_relation = -2	          # negative values mean
					                                  # (max_pred_locks_per_transaction / -max_pred_locks_per_relation) - 1
max_pred_locks_per_page = 8                 # Sets the maximum number of predicate-locked tuples per page
                                            # This controls how many rows on a single page can be predicate-locked before the
                                            # lock is promoted to covering the whole page. The default is 2.
                                            # We increase the row size by 4 times (32k instead of 8k), so increase this too


# Other
synchronize_seqscans = off                  # This allows sequential scans of large tables to synchronize with each other, so
                                            # that concurrent scans read the same block at about the same time and hence share
                                            # the I/O workload.
                                            # This can result in unpredictable changes in the row ordering returned by queries
                                            # that have no ORDER BY clause. Setting this parameter to off ensures the pre-8.3
                                            # behavior in which a sequential scan always starts from the beginning of the table.
                                            # NOTE: We have enough cache (as well in the OS) so that for us order is more important!
transform_null_equals = on                  # Treats "expr=NULL" as "expr IS NULL"
                                            # default is off
