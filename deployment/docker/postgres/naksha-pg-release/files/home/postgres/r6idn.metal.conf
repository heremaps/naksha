# Configuration for Amazon r6idn.metal
# This configuration can be used by simply adding these lines to the postgres.conf:
#
# ----------------------------------------------------------------------------------------

# Memory/Disk
max_connections = 1024
shared_buffers = 512GB
effective_cache_size = 896GB
maintenance_work_mem = 8GB
default_statistics_target = 500
work_mem = 256MB

# WAL
# To change WAL segment size on existing system, do:
# pg_resetwal -D "$PGDATA" --wal-segsize 1GB
wal_level = minimal
wal_compression = lz4    # off, pglz, lz4 (--with-zstd) or zstd (--with-zstd)
full_page_writes = off   # When this parameter is on, the PostgreSQL server writes the entire content
                         # of each disk page to WAL during the first modification of that page after a
                         # checkpoint.
                         # This option assumes that there are none atomic writes, so that partial
                         # writes of a row is possible. However, we aligned the chunk-size (strip-size)
                         # with the row size and that makes is close to impossible to get partial writes
                         # for our EBS volumes. The impact of this setting with our high frequency of
                         # WAL log writes makes it by far to expensive, especially taking into consideration
                         # that we anyway have backups as snapshots on daily base.
#wal_segment_size = 1GB #  The total size of a WAL segment file in bytes (defined in initdb).
wal_buffers = 1GB # The amount of shared memory used for WAL data that has not yet been written to disk (max wal_segment_size).
min_wal_size = 64GB # Minimum WAL size, at least two times wal_segment_size
max_wal_size = 768GB # Maximum WAL size, at least min_wal_size
                      # xlog is started when (max_wal_size / (2 + checkpoint_completion_target)) hit
                      # see: https://github.com/postgres/postgres/blob/REL_16_STABLE/src/backend/access/transam/xlog.c#L1947
                      # #define ConvertToXSegs(x, segsize)	XLogMBVarToSegs((x), (segsize))
                      # ...
                      # static void CalculateCheckpointSegments(void) {
                      # target = (double) ConvertToXSegs(max_wal_size_mb, wal_segment_size) / (1.0 + CheckPointCompletionTarget);
                      # In a nutshell:
                      #     max_wal_size / (1 + checkpoint_completion_target), so around 50% of max_wal_size
                      # This means, to ensure that we do not trigger xlog too early we need to delay it until
                      # the last moment, which is for max_wal_size = (shared_buffers * 1.5), resulting in xlog
                      # starting at (shared_buffers * 0.75)
                      # In our case, xlog we be trigger after around 384gb have been written
                      # (if network is 100% utilized, after around 16 to 20 seconds, roughly)
wal_keep_size = 0 # We do not have replicas
# A checkpoint moves WAL log forward and writes dirty pages.
# We want to write at least 90% of dirty shared_buffers in the checkpoint timeout
# We have 100Gbps EBS bandwidth and 200Gbps network, so theoretically, we can collect 25gb/s of dirty pages
# Assuming we want to bulk load 100 million features with 300gb (around 3kb/feature)
# - We want it to be loaded at ones and then later being written down to EBS
# - We can write 12.5gb/s to WAL and keep all dirty pages in memory
# - It will take only 24 seconds to be loaded, sp 60s is totally long enough, even for slower clients
# Assuming we need to write WAL logs and dirty pages, we have only have 50% of throughput, 6.25gb/s
# - With 6.25gb/s we can write 187.5gb in 30 seconds
checkpoint_completion_target = 0.9
checkpoint_timeout = 60s
# wal-writer runs only when synchronous_commit=off
wal_writer_delay = 1000
wal_writer_flush_after = 1GB
# Write dirty pages in background, reduces load to WAL writer.
# Normally, write 2 times the pages that have been newly allocated up to max-pages.
#bgwriter_delay = 1000ms
#bgwriter_lru_multiplier = 2
bgwriter_lru_maxpages = 0 # Disable background writer, we use only checkpoints, but these more frequently
#bgwriter_flush_after = 0
# disable explicit flush, OS shall do by itself
# write-throughput = (32*320000)/1000 = 10240 mb/s
# total pages = 256*2^30/32,000 = 8,388,608
# write-throughput = ({page-size}*{bgwriter_lru_maxpages})/{bgwriter_delay} = around mb/s
# total pages = {shared_buffers in GiB}*2^20/{page-size}

# Concurrency
effective_io_concurrency = 1000
max_worker_processes = 256
max_parallel_workers = 256
max_parallel_workers_per_gather = 32
max_parallel_maintenance_workers = 8
archive_mode = off
